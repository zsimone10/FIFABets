{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import env.faenv\n",
    "import math\n",
    "\n",
    "import plotting\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FA_Env-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = SGDRegressor(learning_rate=\"constant\")\n",
    "        self.model.partial_fit([self.feature_extractor(env.reset(), 0, 0)], [0]) # initialize to avoid error\n",
    "        \n",
    "        \n",
    "    def featurize(self, features):\n",
    "        return np.array(features)\n",
    "    \n",
    "\n",
    "    def feature_extractor(self, state, bet_amount, bet_team):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"\n",
    "        predictions, odds, cash = state\n",
    "        features = [bet_amount, cash, odds[0], odds[1], odds[2], predictions[0], predictions[1], predictions[2]]\n",
    "        return self.featurize(features)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Makes value function predictions.\n",
    "        \n",
    "        Args:\n",
    "            s: state to make a prediction for\n",
    "            a: (Optional) action to make a prediction for\n",
    "            \n",
    "        Returns\n",
    "            If an action a is given this returns a single number as the prediction.\n",
    "            If no action is given this returns a vector or predictions for all actions\n",
    "            in the environment where pred[i] is the prediction for action i.\n",
    "        \"\"\"\n",
    "        model_pred, odds, cash = s\n",
    "        predictions = np.zeros((math.floor(cash+1), 3))\n",
    "        for bet_amount in np.arange(math.floor(cash + 1 - 1e-10)):\n",
    "            for bet_team in range(3):\n",
    "                features = self.feature_extractor(s, bet_amount, bet_team)\n",
    "                prediction = self.model.predict([features])[0]\n",
    "                predictions[int(bet_amount), bet_team] = prediction\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters for a given state and action towards\n",
    "        the target y.\n",
    "        \"\"\"\n",
    "        bet_amount, bet_team = a\n",
    "        features = self.feature_extractor(s, bet_amount, bet_team)\n",
    "        self.model.partial_fit([features], [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, num_episodes, discount_factor=1.0, epsilon=0.1, epsilon_decay=1.0):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator: Action-Value function estimator\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        epsilon_decay: Each episode, epsilon is decayed by this factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "                \n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        # Also print reward for last episode\n",
    "        last_reward = stats.episode_rewards[i_episode - 1]\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Only used for SARSA, not Q-Learning\n",
    "        next_action = None\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            match_preds, match_odds, cash = state\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = (np.random.randint(cash + 1), np.random.randint(3))\n",
    "            else:\n",
    "                q_values = estimator.predict(state)\n",
    "                best = np.argwhere(q_values.max() == q_values)\n",
    "                chosen = best[np.random.randint(best.shape[0])]\n",
    "                action = (chosen[0], chosen[1])\n",
    "                            \n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "#             print('cash:', cash)\n",
    "#             print('action:', action)\n",
    "#             print('reward:', reward)\n",
    "    \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TD Update\n",
    "            q_values_next = estimator.predict(next_state)\n",
    "            \n",
    "            # Use this code for Q-Learning\n",
    "            # Q-Value TD Target\n",
    "            td_target = reward + discount_factor * np.max(q_values_next)\n",
    "            \n",
    "            # Update the function approximator using our target\n",
    "            estimator.update(state, action, td_target)\n",
    "            \n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(t, i_episode + 1, num_episodes, last_reward), end=\"\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For the Mountain Car we don't actually need an epsilon > 0.0\n",
    "# because our initial estimate for all states is too \"optimistic\" which leads\n",
    "# to the exploration of all states.\n",
    "stats = q_learning(env, estimator, 200, epsilon=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.plot_cost_to_go_mountain_car(env, estimator)\n",
    "plotting.plot_episode_stats(stats, smoothing_window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
